/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
#pragma once

#include <tuple>

#include <executorch/runtime/core/exec_aten/exec_aten.h> // at::Tensor etc.
#include <executorch/codegen/macros.h> // TORCH_API
#include <executorch/runtime/kernel/kernel_runtime_context.h>

// @generated by gen.py from Functions.h

#include "NativeFunctions.h"

namespace torch {
namespace executor {


namespace cortex_m {

// cortex_m::quantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out) {
    return ::cortex_m::native::quantize_per_tensor_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out);
}


// cortex_m::dequantize_per_tensor.out(Tensor input, float scale, int zero_point, int quant_min, int quant_max, ScalarType dtype, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & dequantize_per_tensor_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out) {
    return ::cortex_m::native::dequantize_per_tensor_out(context, input, scale, zero_point, quant_min, quant_max, dtype, out);
}


// cortex_m::quantized_add(Tensor self, Scalar self_zero_point, Scalar self_multiplier, Scalar self_shift, Tensor other, Scalar other_zero_point, Scalar other_multiplier, Scalar other_shift, Scalar output_zero_point, Scalar output_multiplier, Scalar output_shift) -> Tensor
TORCH_API inline torch::executor::Tensor quantized_add(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & self_zero_point, const torch::executor::Scalar & self_multiplier, const torch::executor::Scalar & self_shift, const torch::executor::Tensor & other, const torch::executor::Scalar & other_zero_point, const torch::executor::Scalar & other_multiplier, const torch::executor::Scalar & other_shift, const torch::executor::Scalar & output_zero_point, const torch::executor::Scalar & output_multiplier, const torch::executor::Scalar & output_shift) {
    return ::cortex_m::native::quantized_add(context, self, self_zero_point, self_multiplier, self_shift, other, other_zero_point, other_multiplier, other_shift, output_zero_point, output_multiplier, output_shift);
}


// cortex_m::quantized_add.out(Tensor self, Scalar self_zero_point, Scalar self_multiplier, Scalar self_shift, Tensor other, Scalar other_zero_point, Scalar other_multiplier, Scalar other_shift, Scalar output_zero_point, Scalar output_multiplier, Scalar output_shift, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantized_add_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & self, const torch::executor::Scalar & self_zero_point, const torch::executor::Scalar & self_multiplier, const torch::executor::Scalar & self_shift, const torch::executor::Tensor & other, const torch::executor::Scalar & other_zero_point, const torch::executor::Scalar & other_multiplier, const torch::executor::Scalar & other_shift, const torch::executor::Scalar & output_zero_point, const torch::executor::Scalar & output_multiplier, const torch::executor::Scalar & output_shift, torch::executor::Tensor & out) {
    return ::cortex_m::native::quantized_add_out(context, self, self_zero_point, self_multiplier, self_shift, other, other_zero_point, other_multiplier, other_shift, output_zero_point, output_multiplier, output_shift, out);
}


// cortex_m::quantized_linear(Tensor input, Scalar input_zero_point, Scalar input_multiplier, Scalar input_shift, Tensor weights, Tensor weight_zero_point, Tensor weight_multiplier, Tensor weight_shift, Tensor? bias, Tensor bias_multiplier, Tensor bias_shift, Tensor scratch_buffer, Scalar output_zero_point, Scalar in_features, Scalar out_features) -> Tensor
TORCH_API inline torch::executor::Tensor quantized_linear(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Scalar & input_zero_point, const torch::executor::Scalar & input_multiplier, const torch::executor::Scalar & input_shift, const torch::executor::Tensor & weights, const torch::executor::Tensor & weight_zero_point, const torch::executor::Tensor & weight_multiplier, const torch::executor::Tensor & weight_shift, const torch::executor::optional<torch::executor::Tensor> & bias, const torch::executor::Tensor & bias_multiplier, const torch::executor::Tensor & bias_shift, const torch::executor::Tensor & scratch_buffer, const torch::executor::Scalar & output_zero_point, const torch::executor::Scalar & in_features, const torch::executor::Scalar & out_features) {
    return ::cortex_m::native::quantized_linear(context, input, input_zero_point, input_multiplier, input_shift, weights, weight_zero_point, weight_multiplier, weight_shift, bias, bias_multiplier, bias_shift, scratch_buffer, output_zero_point, in_features, out_features);
}


// cortex_m::quantized_linear.out(Tensor input, Scalar input_zero_point, Scalar input_multiplier, Scalar input_shift, Tensor weights, Tensor weight_zero_point, Tensor weight_multiplier, Tensor weight_shift, Tensor? bias, Tensor bias_multiplier, Tensor bias_shift, Tensor scratch_buffer, Scalar output_zero_point, Scalar in_features, Scalar out_features, *, Tensor(a!) out) -> Tensor(a!)
TORCH_API inline torch::executor::Tensor & quantized_linear_outf(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Scalar & input_zero_point, const torch::executor::Scalar & input_multiplier, const torch::executor::Scalar & input_shift, const torch::executor::Tensor & weights, const torch::executor::Tensor & weight_zero_point, const torch::executor::Tensor & weight_multiplier, const torch::executor::Tensor & weight_shift, const torch::executor::optional<torch::executor::Tensor> & bias, const torch::executor::Tensor & bias_multiplier, const torch::executor::Tensor & bias_shift, const torch::executor::Tensor & scratch_buffer, const torch::executor::Scalar & output_zero_point, const torch::executor::Scalar & in_features, const torch::executor::Scalar & out_features, torch::executor::Tensor & out) {
    return ::cortex_m::native::quantized_linear_out(context, input, input_zero_point, input_multiplier, input_shift, weights, weight_zero_point, weight_multiplier, weight_shift, bias, bias_multiplier, bias_shift, scratch_buffer, output_zero_point, in_features, out_features, out);
}

} // namespace cortex_m

} // namespace executor
} // namespace torch
